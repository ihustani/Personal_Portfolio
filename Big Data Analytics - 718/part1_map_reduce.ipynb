{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0a8b253cea124309dc7872a31b41818e",
     "grade": false,
     "grade_id": "cell-b038e38b5e3072a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Professor: Daniel Acuna <deacuna@syr.edu>\n",
    "\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers either from your classmates or from the internet__\n",
    "- You can put the homework files anywhere you want in your https://jupyterhub.ischool.syr.edu/ workspace but _do not change_ the file names. The TAs and the professor use these names to grade your homework.\n",
    "- Remove or comment out code that contains `raise NotImplementedError`. This is mainly to make the `assert` statement fail if nothing is submitted.\n",
    "- The tests shown in some cells (i.e., `assert` and `np.testing.` statements) are used to grade your answers. **However, the professor and TAs will use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
    "- Before downloading and submitting your work through Blackboard, remember to save and press `Validate` (or go to \n",
    "`Kernel`$\\rightarrow$`Restart and Run All`). \n",
    "- Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: MapReduce with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2b655915b932d14b625460b783c01b66",
     "grade": false,
     "grade_id": "cell-7cd9ec3db4973169",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this code to create the Spark session\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('map-reduce').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "18981a3fe5a85964b3d6cb3130b74650",
     "grade": false,
     "grade_id": "cell-5b467aa4e5cdbe39",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# this RDD will be used throughout this part of the homework\n",
    "gpa_rdd = sc.parallelize([\n",
    " ['2015', 'Fall', 'IST101', 1, 'A', 4],\n",
    " ['2015', 'Fall', 'IST195', 3, 'A', 4],\n",
    " ['2015', 'Fall', 'IST233', 3, 'B+', 3.3],\n",
    " ['2015', 'Fall', 'SOC101', 3, 'A-', 3.7],\n",
    " ['2015', 'Fall', 'MAT221', 3, 'C', 2],\n",
    " ['2016', 'Fall', 'IST346', 3, 'A', 4],\n",
    " ['2016', 'Fall', 'CHE111', 4, 'A-', 3.7],\n",
    " ['2016', 'Fall', 'PSY120', 3, 'B+', 3.3],\n",
    " ['2016', 'Fall', 'IST256', 3, 'A', 4],\n",
    " ['2016', 'Fall', 'ENG121', 3, 'B+', 3.3],\n",
    " ['2016', 'Spring', 'GEO110', 3, 'B+', 3.3],\n",
    " ['2016', 'Spring', 'MAT222', 3, 'A', 4],\n",
    " ['2016', 'Spring', 'SOC121', 3, 'C+', 2.3],\n",
    " ['2016', 'Spring', 'BIO240', 3, 'B-', 2.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 Cumulative GPA with MapReduce (15 pts)\n",
    "\n",
    "The cumulative GPA reported in any transcript is the average of the numerical score of a course weighted by the credits of such course. Construct a MapReduce job that takes the `gpa_rdd` RDD and returns the cumulative GPA *per season*. \n",
    "\n",
    "Each record in `gpa_rdd` contains:\n",
    "- The year\n",
    "- The season\n",
    "- The course code\n",
    "- The number of credits\n",
    "- The letter grade\n",
    "- The number grade\n",
    "\n",
    "**Hint:** In class, we discussed the MapReduce job for computing avereage. In this case, the key-value pairs will be similar but instead of counting the number of elements considered in the average so far, we need to count the credits. Clearly, the key needs to be the semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b9838d4d7ba5c693e77b11ccade70d0f",
     "grade": false,
     "grade_id": "cell-9cd285edff34dc29",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def map_weighted_gpa(record): # Defined map_weighted_gpa() map function\n",
    "    year, season, course_code, number_of_credits, letter_grade, number_grade = record # Defined what each record contains\n",
    "    c = number_of_credits # Assigned number_of_credits as c\n",
    "    g = number_grade # Assigned number_grade as g\n",
    "    return [season, [g,c]] # Returned season as key, with g and c as a tuple of values\n",
    "\n",
    "def reduce_weighted_gpa(value1, value2): # Defined reduce_weighted_gpa() reduce function\n",
    "    g1, c1 = value1 # Defined what value 1 contains\n",
    "    g2, c2 = value2 # Defined what value 2 contains\n",
    "    c3 = c1 + c2 # Total number of credits is equal to the number of credits from both value1 and value 2\n",
    "    g3 = (g1*c1 + g2*c2)/c3 # Average equals the grade multipled by the number of credits from value 1 added to the grade multipled by the number of credits from value 2 divided by the total number of credits\n",
    "    return [ \n",
    "        g3\n",
    "        ,\n",
    "        c3\n",
    "    ] # Returned weighted gpa and number of credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map job should produce as key the season and value a tuple or list with the grade and credit.\n",
    "\n",
    "For example, the first element of the map of `gpa_rdd` should be\n",
    "\n",
    "```python\n",
    "gpa_rdd.\\\n",
    "    map(map_weighted_gpa).\\\n",
    "    first()\n",
    "```\n",
    "\n",
    "```python\n",
    "['Fall', [4, 1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fall', [4, 1]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpa_rdd.\\\n",
    "    map(map_weighted_gpa).\\\n",
    "    first()\n",
    "['Fall', [4, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "35670b1587138e4b26dc13880475a50e",
     "grade": true,
     "grade_id": "cell-cb3fc3eeaf78b636",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 15 pts\n",
    "##### first result\n",
    "assert gpa_rdd.\\\n",
    "    map(map_weighted_gpa).\\\n",
    "    first() == ['Fall', [4, 1]]\n",
    "# the key should be a string\n",
    "assert gpa_rdd.map(map_weighted_gpa).map(lambda x: type(x[0])).distinct().count() == 1\n",
    "assert gpa_rdd.map(map_weighted_gpa).map(lambda x: type(x[0])).distinct().first() == str\n",
    "# all values should be of length 2\n",
    "assert gpa_rdd.map(map_weighted_gpa).map(lambda x: len(x[1])).distinct().count() == 1\n",
    "assert gpa_rdd.map(map_weighted_gpa).map(lambda x: len(x[1])).distinct().first() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fall', [3.503448275862069, 29]), ('Spring', [3.0749999999999997, 12])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there should be two results in the map reduce because there are two semesters\n",
    "gpa_rdd.\\\n",
    "    map(map_weighted_gpa).\\\n",
    "    reduceByKey(reduce_weighted_gpa).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1800dc9bc4cd97d1fba1ef40f4fbe94b",
     "grade": true,
     "grade_id": "cell-8076e4e7419738f4",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 5 pts\n",
    "assert (gpa_rdd.\\\n",
    "    map(map_weighted_gpa).\\\n",
    "    reduceByKey(reduce_weighted_gpa).collect() == \\\n",
    "    [('Spring', [3.0749999999999997, 12]), \n",
    "     ('Fall', [3.503448275862069, 29])]) or \\\n",
    "    (gpa_rdd.\\\n",
    "    map(map_weighted_gpa).\\\n",
    "    reduceByKey(reduce_weighted_gpa).collect() == \\\n",
    "    [('Fall', [3.503448275862069, 29]),\n",
    "     ('Spring', [3.0749999999999997, 12])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2 Time series\n",
    "\n",
    "Historical changes in temperature throughout history is a higly noisy process. Still, we can use long-term trends to understand these fluctiations and perhaps relate them to external factors such as changes in the oceans, magnetic fields of the earth, or human activity (CO2).\n",
    "\n",
    "Luckily, we have data to explore how temperatures have changes over the years. In the dataset below, we have obtained a set of key-value pairs where the key is the year $y$ and the value is a temperature deviation $t_y$ from a long-term historical record (e.g., inferred from the artic's permafrost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a98857e9fdd1cc01f83c32be970113c4",
     "grade": false,
     "grade_id": "cell-d5f63252a7705a00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "global_avg_temp = sc.parallelize([[1880, -0.06], [1881, -0.09], [1882, 0.08], [1883, -0.28], \n",
    "                                  [1884, -0.24], \n",
    "                   [1885, -0.45], [1886, -0.18], [1887, -0.53], [1888, -0.33], [1889, -0.03],\n",
    "                   [1890, -0.34], [1891, -0.4], [1892, -0.25], [1893, -0.74], [1894, -0.42],\n",
    "                   [1895, -0.44], [1896, -0.15], [1897, -0.13], [1898, -0.01], [1899, -0.14],\n",
    "                   [1900, -0.35], [1901, -0.13], [1902, -0.1], [1903, -0.24], [1904, -0.62],\n",
    "                   [1905, -0.35], [1906, -0.14], [1907, -0.37], [1908, -0.38], [1909, -0.58],\n",
    "                    [1910, -0.36], [1911, -0.55], [1912, -0.28], [1913, -0.35], [1914, 0.11],\n",
    "                   [1915, -0.15], [1916, -0.13], [1917, -0.53], [1918, -0.32], [1919, -0.23],\n",
    "                    [1920, -0.2], [1921, -0.07], [1922, -0.3], [1923, -0.24], [1924, -0.22],\n",
    "                    [1925, -0.4], [1926, 0.13], [1927, -0.23], [1928, -0.08], [1929, -0.49],\n",
    "                    [1930, -0.34], [1931, -0.03], [1932, 0.15], [1933, -0.29], [1934, -0.3],\n",
    "                    [1935, -0.28], [1936, -0.27], [1937, -0.18], [1938, 0.02], [1939, -0.11],\n",
    "                    [1940, 0.01], [1941, 0.27], [1942, 0.29], [1943, -0.03], [1944, 0.39],\n",
    "                    [1945, 0.16], [1946, 0.24], [1947, -0.16], [1948, 0.06], [1949, 0.12],\n",
    "                   [1950, -0.29], [1951, -0.28], [1952, 0.13], [1953, 0.09], [1954, -0.2],\n",
    "                    [1955, 0.14], [1956, -0.14], [1957, -0.09], [1958, 0.32], [1959, 0.16],\n",
    "                   [1960, 0.04], [1961, 0.13], [1962, 0.12], [1963, 0.06], [1964, 0.01],\n",
    "                    [1965, -0.07], [1966, -0.05], [1967, -0.09], [1968, -0.2], [1969, -0.09],\n",
    "                    [1970, 0.14], [1971, 0.01], [1972, -0.24], [1973, 0.28], [1974, -0.19],\n",
    "                    [1975, 0.11], [1976, -0.02], [1977, 0.13], [1978, 0.16], [1979, 0.15],\n",
    "                    [1980, 0.33], [1981, 0.51], [1982, 0.14], [1983, 0.53], [1984, 0.3],\n",
    "                    [1985, 0.22], [1986, 0.31], [1987, 0.32], [1988, 0.56], [1989, 0.17],\n",
    "                    [1990, 0.36], [1991, 0.43], [1992, 0.46], [1993, 0.36], [1994, 0.27],\n",
    "                    [1995, 0.56], [1996, 0.25], [1997, 0.34], [1998, 0.6], [1999, 0.51],\n",
    "                   [2000, 0.34], [2001, 0.47], [2002, 0.71], [2003, 0.72], [2004, 0.61],\n",
    "                    [2005, 0.65], [2006, 0.51], [2007, 0.92], [2008, 0.27], [2009, 0.6],\n",
    "                    [2010, 0.73], [2011, 0.47], [2012, 0.44], [2013, 0.62],\n",
    "                   [2014, 0.7], [2015, 0.83], [2016, 1.12], [2017, 0.98], [2018, 0.76],\n",
    "                   [2019, 0.94], [2020, 1.15], [2021, 0.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2.1: Year with lowest temperature deviation (10 pts)\n",
    "\n",
    "In this question, you will compute the year with the minimum temperature deviation using MapReduce. The key-value pair structure of the map reduce should have a single key and you should use the string `\"min\"` for it. The first element of the value at the end of the computation should have the year and the second element should be the lowest temperature deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "65543717308b2de73e10edf70ee3fc2e",
     "grade": false,
     "grade_id": "cell-657d7ee252c0aded",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define the map operation\n",
    "def argmin_map(kv): # Defined argin_map() map function\n",
    "    year, deviation = kv # Defined what each key value pair contains\n",
    "    y = year # Defined year as y\n",
    "    d = deviation # Defined deviation as d\n",
    "    return [\"min\", [y,d]] # Returned \"min\" as key, with y and d as a tuple of values\n",
    "\n",
    "def argmin_reduce(v1, v2): # Defined argmin_reduce() reduce function\n",
    "    y1, d1 = v1 # Defined what v1 contains\n",
    "    y2, d2 = v2 # Defined what v2 contains\n",
    "    d3 = min(d1,d2) # Set d3 as the lower of the two deviations using the min() function\n",
    "    if d3 == d1: # If d3 equals d1:\n",
    "        y3 = y1 # Assigned y1 as the minimum year\n",
    "    else: # Else:\n",
    "        y3 = y2 # Assigned y2 as the minimum year\n",
    "    return [  \n",
    "        y3\n",
    "        ,\n",
    "        d3\n",
    "    ] # Returned year with minimum deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('min', [1893, -0.74])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_avg_temp.map(argmin_map).reduceByKey(argmin_reduce).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0306c171514e410bcc5425269714958a",
     "grade": true,
     "grade_id": "cell-cb4d0b779d226660",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 10 pts\n",
    "# there should be 1 result only\n",
    "assert global_avg_temp.map(argmin_map).reduceByKey(argmin_reduce).count() == 1\n",
    "# the resulting value should have two elements (year, lowest temperature)\n",
    "assert len(global_avg_temp.map(argmin_map).reduceByKey(argmin_reduce).values().first()) == 2\n",
    "# lowest temperature happened in year 1893\n",
    "assert global_avg_temp.map(argmin_map).reduceByKey(argmin_reduce).values().first()[0] == 1893\n",
    "# lowest temperature was -0.74\n",
    "assert global_avg_temp.map(argmin_map).reduceByKey(argmin_reduce).values().first()[1] == -0.74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2.2: Running average of temperature deviation (25 pts)\n",
    "\n",
    "Sometimes, time series are too noisy and require smoothing. We can do this smoothing by computing a running average of values. In this question, you will compute a simple moving average where the resulting RDD will have as key the year $y$ and as the first element of the value $m_y$, the average temperature deviation of the last $n$ years in the data (including the current year):\n",
    "\n",
    "$$m_y = \\frac{\\sum_{i=y-n+1}^{y} t_i}{n}$$\n",
    "\n",
    "For example, for the input data and for $n=2$\n",
    "\n",
    "```\n",
    "(1880, -0.06)\n",
    "(1881, -0.09)\n",
    "(1882, 0.08)\n",
    "(1883, -0.28)\n",
    "```\n",
    "\n",
    "The first element for key 1883 will be\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "mm_{1883} &=\\frac{\\sum_{i=1883-2+1}^{1883} t_i}{2}\\\\\n",
    "& =\\frac{\\sum_{i=1882}^{1883} t_i}{2}\\\\\n",
    "& =\\frac{0.08 -0.28}{2}\\\\\n",
    "& =0.1\n",
    "\\end{array}$$\n",
    "\n",
    "Below, define a function that computes the moving average for $n=5$. Notice that you will be able to compute the moving average **only** starting in year 1884 and ending in year 2021. Because each input element should likely generate several key-value pairs, use `flatMap` instead of `map` as your map step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "edcd2e047150ce3759b2a57bda770bc2",
     "grade": false,
     "grade_id": "cell-7f457aba82664e70",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define the map operation\n",
    "def ma_map(kv): # Defined ma_map() map function\n",
    "    n = 5 # Set number of years as equal to 5\n",
    "    x = 1 # Set default count as 1\n",
    "    deviation = [] # Created empty deviation list\n",
    "    for year in range(n): # For year in range(n) 1,2,3,4,5...\n",
    "        deviation.append([kv[0] + year, [kv[1], x]]) # Append the first value of the key,value pair adding each value of n as the key and the second value of the key,value pair along with x (1) as a tuple of values\n",
    "    return(deviation) # Returned year as key, with deviation and count as a tuple of values over the course of 5 years\n",
    "\n",
    "def ma_reduce(v1, v2): # Defined ma_reduce() reduce function\n",
    "    g1, c1 = v1 # Defined what v1 contains\n",
    "    g2, c2 = v2 # Defined what v2 contains\n",
    "    c3 = c1 + c2 # Used same average function as used in question 1.1\n",
    "    g3 = (g1*c1 + g2*c2)/c3 \n",
    "    return [ \n",
    "        g3\n",
    "        ,\n",
    "        c3\n",
    "    ] # Returned moving average and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11800000000000002"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first result of the moving average should be average of the first five values of the temperature deviations\n",
    "sum(global_avg_temp.values().take(5))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1884, -0.118)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that you get the same results\n",
    "sc.parallelize(global_avg_temp.take(5)).\\\n",
    "    flatMap(ma_map).\\\n",
    "    reduceByKey(ma_reduce).\\\n",
    "    filter(lambda kv: kv[0] == 1884).\\\n",
    "    map(lambda kv: (kv[0], kv[1][0])).\\\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were to run the following code, it should love like this:\n",
    "\n",
    "```python\n",
    "global_avg_temp.\\\n",
    "    flatMap(ma_map).\\\n",
    "    reduceByKey(ma_reduce).\\\n",
    "    filter(lambda kv: kv[0] >= 1884 and kv[0] <= 2021).\\\n",
    "    map(lambda kv: [kv[0], kv[1][0]]).\\\n",
    "    sortByKey().\\\n",
    "    take(10)\n",
    "```\n",
    "\n",
    "```\n",
    "[(1884, -0.11800000000000002),\n",
    " (1885, -0.196),\n",
    " (1886, -0.21400000000000002),\n",
    " (1887, -0.33599999999999997),\n",
    " (1888, -0.346),\n",
    " (1889, -0.30400000000000005),\n",
    " (1890, -0.28200000000000003),\n",
    " (1891, -0.32600000000000007),\n",
    " (1892, -0.27),\n",
    " (1893, -0.352)]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1884, -0.11800000000000002),\n",
       " (1885, -0.196),\n",
       " (1886, -0.21400000000000002),\n",
       " (1887, -0.33599999999999997),\n",
       " (1888, -0.346),\n",
       " (1889, -0.30400000000000005),\n",
       " (1890, -0.28200000000000003),\n",
       " (1891, -0.32600000000000007),\n",
       " (1892, -0.27),\n",
       " (1893, -0.352)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try it here\n",
    "global_avg_temp.\\\n",
    "    flatMap(ma_map).\\\n",
    "    reduceByKey(ma_reduce).\\\n",
    "    filter(lambda kv: kv[0] >= 1884 and kv[0] <=2021).\\\n",
    "    map(lambda kv: [kv[0], kv[1][0]]).\\\n",
    "    sortByKey().\\\n",
    "    take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9d32708ec91f81a3edbbf4bb1327ca96",
     "grade": true,
     "grade_id": "cell-81046f3d54bd6944",
     "locked": true,
     "points": 25,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 25 pts\n",
    "np.testing.assert_almost_equal(global_avg_temp.\\\n",
    "    flatMap(ma_map).\\\n",
    "    reduceByKey(ma_reduce).\\\n",
    "    filter(lambda kv: kv[0] >= 1884 and kv[0] <= 2021).\\\n",
    "    map(lambda kv: [kv[0], kv[1][0]]).\\\n",
    "    sortByKey().\\\n",
    "    values().sum(), 7.807999999999996)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
